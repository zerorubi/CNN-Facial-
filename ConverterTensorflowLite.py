"""
ğŸ“± CONVERTIDOR A TENSORFLOW LITE
=================================
Convierte tu modelo entrenado a formato .tflite optimizado para mÃ³viles
Incluye: cuantizaciÃ³n, optimizaciÃ³n y pruebas de latencia
"""

import os
import numpy as np
import tensorflow as tf
from pathlib import Path
import json
import time

print("=" * 80)
print("ğŸ“± CONVERTIDOR A TENSORFLOW LITE")
print("=" * 80)
print(f"TensorFlow: {tf.__version__}")
print("=" * 80)


class TFLiteConverter:
    """
    Convierte modelo Keras a TensorFlow Lite optimizado
    """

    def __init__(self, model_path, output_path, class_names):
        """
        Args:
            model_path: Ruta al modelo .h5
            output_path: Carpeta para guardar .tflite
            class_names: Lista de nombres de clases
        """
        self.model_path = Path(model_path)
        self.output_path = Path(output_path)
        self.output_path.mkdir(parents=True, exist_ok=True)
        self.class_names = class_names

        print(f"\nğŸ“‚ Modelo origen: {self.model_path}")
        print(f"ğŸ“‚ Output: {self.output_path}")
        print(f"ğŸ‘¥ Clases: {self.class_names}")

    def load_model(self):
        """Carga el modelo Keras"""
        print("\nğŸ”„ Cargando modelo...")
        self.model = tf.keras.models.load_model(self.model_path)
        print("âœ… Modelo cargado")
        self.model.summary()
        return self.model

    def convert_to_tflite(self, quantization='float16'):
        """
        Convierte a TensorFlow Lite

        Args:
            quantization: 'none', 'float16', 'int8'
                - 'none': Sin cuantizaciÃ³n (mÃ¡s preciso, mÃ¡s pesado)
                - 'float16': CuantizaciÃ³n a 16 bits (RECOMENDADO)
                - 'int8': CuantizaciÃ³n a 8 bits (mÃ¡s ligero, menos preciso)
        """
        print(f"\nğŸ”§ Convirtiendo a TFLite (cuantizaciÃ³n: {quantization})...")

        # Crear convertidor
        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)

        # Aplicar optimizaciones
        converter.optimizations = [tf.lite.Optimize.DEFAULT]

        if quantization == 'float16':
            # CuantizaciÃ³n a float16 (RECOMENDADO)
            converter.target_spec.supported_types = [tf.float16]
            output_name = 'model_float16.tflite'

        elif quantization == 'int8':
            # CuantizaciÃ³n completa a int8 (necesita datos representativos)
            print("   âš ï¸ int8 requiere dataset representativo")
            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
            converter.inference_input_type = tf.uint8
            converter.inference_output_type = tf.uint8
            output_name = 'model_int8.tflite'

        else:
            # Sin cuantizaciÃ³n
            output_name = 'model.tflite'

        # Convertir
        tflite_model = converter.convert()

        # Guardar
        tflite_path = self.output_path / output_name
        with open(tflite_path, 'wb') as f:
            f.write(tflite_model)

        # Obtener tamaÃ±o
        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)

        print(f"âœ… Modelo convertido:")
        print(f"   ğŸ“„ Archivo: {output_name}")
        print(f"   ğŸ’¾ TamaÃ±o: {size_mb:.2f} MB")

        return tflite_path, size_mb

    def test_tflite_inference(self, tflite_path, test_image):
        """
        Prueba inferencia con TFLite y mide latencia

        Args:
            tflite_path: Ruta al modelo .tflite
            test_image: Imagen de prueba (numpy array)
        """
        print(f"\nğŸ§ª Probando inferencia con TFLite...")

        # Cargar intÃ©rprete TFLite
        interpreter = tf.lite.Interpreter(model_path=str(tflite_path))
        interpreter.allocate_tensors()

        # Obtener detalles de input/output
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()

        print(f"   ğŸ“¥ Input shape: {input_details[0]['shape']}")
        print(f"   ğŸ“¤ Output shape: {output_details[0]['shape']}")

        # Preparar imagen
        input_shape = input_details[0]['shape']

        if test_image.shape[0:3] != tuple(input_shape[1:4]):
            print(f"   âš ï¸ Redimensionando imagen a {input_shape[1:3]}")
            test_image = tf.image.resize(test_image, input_shape[1:3]).numpy()

        # Expandir dimensiones si es necesario
        if len(test_image.shape) == 3:
            test_image = np.expand_dims(test_image, axis=0)

        # Asegurar tipo correcto
        input_dtype = input_details[0]['dtype']
        if input_dtype == np.uint8:
            test_image = test_image.astype(np.uint8)
        else:
            test_image = test_image.astype(np.float32)

        # Medir latencia (mÃºltiples iteraciones)
        print(f"\nâ±ï¸  Midiendo latencia (10 inferencias)...")
        latencies = []

        for i in range(10):
            start = time.time()

            interpreter.set_tensor(input_details[0]['index'], test_image)
            interpreter.invoke()
            output = interpreter.get_tensor(output_details[0]['index'])

            end = time.time()
            latency_ms = (end - start) * 1000
            latencies.append(latency_ms)

        avg_latency = np.mean(latencies)
        std_latency = np.std(latencies)

        print(f"   âœ… Latencia promedio: {avg_latency:.2f} ms (Â±{std_latency:.2f} ms)")
        print(f"   ğŸš€ FPS estimado: {1000/avg_latency:.1f}")

        # Obtener predicciÃ³n
        predicted_class = np.argmax(output[0])
        confidence = output[0][predicted_class]

        print(f"\nğŸ¯ PredicciÃ³n de prueba:")
        print(f"   Clase: {self.class_names[predicted_class]}")
        print(f"   Confianza: {confidence:.2%}")

        return {
            'avg_latency_ms': float(avg_latency),
            'std_latency_ms': float(std_latency),
            'fps': float(1000/avg_latency),
            'predicted_class': self.class_names[predicted_class],
            'confidence': float(confidence)
        }

    def save_labels_file(self):
        """Guarda archivo de labels para la app mÃ³vil"""
        labels_path = self.output_path / 'labels.txt'

        with open(labels_path, 'w', encoding='utf-8') as f:
            for class_name in self.class_names:
                f.write(f"{class_name}\n")

        print(f"\nğŸ“„ Archivo de labels guardado: labels.txt")
        return labels_path

    def create_metadata_json(self, tflite_info):
        """Crea archivo de metadatos para la app"""
        metadata = {
            'model_name': self.model_path.stem,
            'num_classes': len(self.class_names),
            'class_names': self.class_names,
            'input_shape': self.model.input_shape[1:],
            'tflite_model': tflite_info['filename'],
            'model_size_mb': tflite_info['size_mb'],
            'avg_latency_ms': tflite_info.get('avg_latency_ms', 0),
            'preprocessing': {
                'rescale': '1/255',
                'input_type': 'uint8 or float32'
            }
        }

        metadata_path = self.output_path / 'model_metadata.json'
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=4)

        print(f"ğŸ“„ Metadatos guardados: model_metadata.json")
        return metadata_path

    def convert_all_versions(self):
        """
        Convierte el modelo a mÃºltiples versiones optimizadas
        """
        print("\n" + "=" * 80)
        print("ğŸ”„ GENERANDO TODAS LAS VERSIONES")
        print("=" * 80)

        results = []

        # VersiÃ³n 1: Sin cuantizaciÃ³n
        print("\nğŸ“¦ VersiÃ³n 1: Sin cuantizaciÃ³n (mÃ¡s precisa)")
        path1, size1 = self.convert_to_tflite('none')
        results.append({
            'version': 'sin_cuantizacion',
            'filename': path1.name,
            'size_mb': size1,
            'path': str(path1)
        })

        # VersiÃ³n 2: Float16 (RECOMENDADA)
        print("\nğŸ“¦ VersiÃ³n 2: Float16 (RECOMENDADA)")
        path2, size2 = self.convert_to_tflite('float16')
        results.append({
            'version': 'float16',
            'filename': path2.name,
            'size_mb': size2,
            'path': str(path2)
        })

        # Resumen
        print("\n" + "=" * 80)
        print("ğŸ“Š RESUMEN DE VERSIONES")
        print("=" * 80)

        for result in results:
            print(f"\n{result['version'].upper()}:")
            print(f"   ğŸ“„ Archivo: {result['filename']}")
            print(f"   ğŸ’¾ TamaÃ±o: {result['size_mb']:.2f} MB")

        return results

    def run_full_conversion(self, test_image_path=None):
        """
        Pipeline completo de conversiÃ³n
        """
        print("\n" + "ğŸš€" * 40)
        print("CONVERSIÃ“N COMPLETA A TENSORFLOW LITE")
        print("ğŸš€" * 40)

        # 1. Cargar modelo
        self.load_model()

        # 2. Convertir todas las versiones
        versions = self.convert_all_versions()

        # 3. Guardar labels
        self.save_labels_file()

        # 4. Probar inferencia (versiÃ³n float16)
        test_results = None
        if test_image_path and Path(test_image_path).exists():
            print("\nğŸ“¸ Cargando imagen de prueba...")
            test_img = tf.keras.preprocessing.image.load_img(
                test_image_path,
                target_size=self.model.input_shape[1:3]
            )
            test_img = tf.keras.preprocessing.image.img_to_array(test_img)

            # Probar con versiÃ³n float16
            float16_path = self.output_path / 'model_float16.tflite'
            test_results = self.test_tflite_inference(float16_path, test_img)

            # Agregar resultados a metadata
            versions[1]['avg_latency_ms'] = test_results['avg_latency_ms']
            versions[1]['fps'] = test_results['fps']

        # 5. Crear metadatos
        self.create_metadata_json(versions[1])  # Usar versiÃ³n float16

        print("\n" + "ğŸ‰" * 40)
        print("âœ… CONVERSIÃ“N COMPLETADA")
        print("ğŸ‰" * 40)

        print(f"\nğŸ“‚ Archivos generados en: {self.output_path}")
        print(f"   â€¢ model.tflite (sin cuantizaciÃ³n)")
        print(f"   â€¢ model_float16.tflite (RECOMENDADO)")
        print(f"   â€¢ labels.txt")
        print(f"   â€¢ model_metadata.json")

        return versions


# ============================================================================
# SCRIPT DE USO
# ============================================================================

if __name__ == "__main__":
    # Montar Google Drive
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        print("âœ… Google Drive montado")
    except:
        print("âš ï¸ No se estÃ¡ en Colab")

    print("\n" + "ğŸ“±" * 40)
    print("CONVERTIDOR A TENSORFLOW LITE PARA MÃ“VILES")
    print("ğŸ“±" * 40)

    # âš™ï¸ CONFIGURACIÃ“N - MODIFICA ESTAS RUTAS
    MODEL_PATH = "/content/drive/MyDrive/resultados_proyecto_cnn/mobilenetv2/model_mobilenetv2.h5"
    OUTPUT_PATH = "/content/drive/MyDrive/modelo_tflite"

    # Cargar nombres de clases desde metadata
    import json
    metadata_path = "/content/drive/MyDrive/fotitos_procesadas/metadata.json"

    if Path(metadata_path).exists():
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
            CLASS_NAMES = metadata['persons']
    else:
        # Si no existe metadata, definir manualmente
        CLASS_NAMES = ['Persona1', 'Persona2', 'Persona3']  # âš ï¸ MODIFICA ESTO
        print("âš ï¸ No se encontrÃ³ metadata.json, usando nombres por defecto")

    print(f"\nğŸ‘¥ Clases detectadas: {CLASS_NAMES}")

    # Verificar que existe el modelo
    if not Path(MODEL_PATH).exists():
        print(f"\nâŒ ERROR: No se encuentra el modelo en {MODEL_PATH}")
        print("ğŸ’¡ AsegÃºrate de haber entrenado el modelo primero")
    else:
        print(f"\nâœ… Modelo encontrado: {MODEL_PATH}")

        # Crear convertidor
        converter = TFLiteConverter(
            model_path=MODEL_PATH,
            output_path=OUTPUT_PATH,
            class_names=CLASS_NAMES
        )

        # Ejecutar conversiÃ³n completa
        # Si tienes una imagen de prueba, pÃ¡sala aquÃ­:
        # TEST_IMAGE = "/ruta/a/imagen/prueba.jpg"
        TEST_IMAGE = None

        versions = converter.run_full_conversion(test_image_path=TEST_IMAGE)

        print("\n" + "=" * 80)
        print("ğŸ“± SIGUIENTE PASO: IMPLEMENTAR APP MÃ“VIL")
        print("=" * 80)
        print("\nğŸ’¡ Archivos que necesitas copiar a tu app:")
        print(f"   1. {OUTPUT_PATH}/model_float16.tflite  (modelo optimizado)")
        print(f"   2. {OUTPUT_PATH}/labels.txt             (nombres de clases)")

        print("\nğŸ“‹ Especificaciones para la app:")
        print(f"   â€¢ NÃºmero de clases: {len(CLASS_NAMES)}")
        print(f"   â€¢ Input shape: {converter.model.input_shape[1:]}")
        print(f"   â€¢ Preprocesamiento: Rescale 1/255")

        if versions[1].get('avg_latency_ms'):
            print(f"   â€¢ Latencia esperada: {versions[1]['avg_latency_ms']:.2f} ms")
            print(f"   â€¢ FPS esperado: {versions[1]['fps']:.1f}")

        print("\nğŸ“ Â¡Listo para implementar en la app mÃ³vil!")
